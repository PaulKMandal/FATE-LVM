{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visual Transformer Training with Parameter Efficient methods in FATE-ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we train a federated ViT on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details of FATE-LLM dataset setting, we recommend that you read through these tutorials first: [NN Dataset Customization](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Homo-NN-Customize-your-Dataset.ipynb), [Some Built-In Dataset](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Introduce-Built-In-Dataset.ipynb),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PELLM Model in FATE with CustModel\n",
    "\n",
    "In this [Model Customization](https://github.com/FederatedAI/FATE/blob/master/doc/tutorial/pipeline/nn_tutorial/Homo-NN-Customize-Model.ipynb) tutorial, we demonstrate how to employ the t.nn.CustomModel class in fate_torch to parse a model's structure and submit it to a federated learning task. The CustomModel automatically imports the model class from the model_zoo and initializes the models with the parameters provided. Since these language models are built-in, we can directly use them in the CustomModel and easily add a classifier head to address the classification task at handï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component.nn import save_to_fate_llm\n",
    "fate_torch_hook(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate_llm model sigmoid.py\n",
    "\n",
    "import torch as t\n",
    "\n",
    "class Sigmoid(t.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = t.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(x.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Federated Task\n",
    "Once you have successfully completed local testing, We can submit a task to FATE. Please notice that this tutorial is ran on a standalone version. **Please notice that in this tutorial we are using a standalone version, if you are using a cluster version, you need to bind the data with the corresponding name&namespace on each machine.**\n",
    "\n",
    "In this example we load pretrained weights for gpt2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224',\n",
    "    num_labels=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fate_llm.dataset.image_tokenizer import TokenizerImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TokenizerImageDataset()\n",
    "\n",
    "fate_path = '/data/projects/fate'\n",
    "path=fate_path + '/examples/data/cifar10/test'\n",
    "\n",
    "test.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import os\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.component.homo_nn import DatasetParam, TrainerParam\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader\n",
    "from pipeline.interface import Data\n",
    "from transformers import ViTConfig\n",
    "\n",
    "\n",
    "fate_torch_hook(t)\n",
    "\n",
    "\n",
    "import os\n",
    "fate_project_path = '/data/projects/fate'\n",
    "guest = 9999\n",
    "host = 9999\n",
    "\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest).set_roles(guest=guest, host=host,\n",
    "                                                                            arbiter=host)\n",
    "data_0 = {\"name\": \"cifar10\", \"namespace\": \"experiment\"}\n",
    "data_path = fate_project_path + '/examples/data/cifar10/train'\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest).component_param(table=data_0)\n",
    "reader_0.get_party_instance(role='host', party_id=host).component_param(table=data_0)\n",
    "\n",
    "#reader_1 = Reader(name=\"reader_1\")\n",
    "#reader_1.get_party_instance(role='guest', party_id=guest).component_param(table=data_0)\n",
    "#reader_1.get_party_instance(role='host', party_id=host).component_param(table=data_0)\n",
    "## Add your pretriained model path here, will load model&tokenizer from this path\n",
    "\n",
    "\n",
    "## LoraConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "\"\"\"\n",
    "    LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    #target_modules=['c_attn']\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_path = 'google/vit-base-patch16-224'\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.vit', class_name='vit', pretrained_path=model_path,\n",
    "                   peft_config=lora_config.to_dict(), peft_type=\"LoraConfig\", num_labels=1000,  pad_token_id=50256),\n",
    "    t.nn.CustModel(module_name='sigmoid', class_name='Sigmoid')\n",
    ")\n",
    "\n",
    "# DatasetParam\n",
    "dataset_param = DatasetParam(dataset_name='image_tokenizer') #\n",
    "#DatasetParam(dataset_name='nlp_tokenizer',text_max_length=128, tokenizer_name_or_path=model_path, \n",
    "#                             padding_side=\"left\", return_input_ids=False, pad_token='<|endoftext|>')\n",
    "# TrainerParam\n",
    "trainer_param = TrainerParam(trainer_name='fedavg_vit_trainer', epochs=1, batch_size=8,\n",
    "                             data_loader_worker=1)\n",
    "\n",
    "nn_component = HomoNN(name='nn_0', model=model)\n",
    "\n",
    "# set parameter for client 1\n",
    "nn_component.get_party_instance(role='guest', party_id=guest).component_param(\n",
    "    loss=t.nn.CrossEntropyLoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for client 2\n",
    "nn_component.get_party_instance(role='host', party_id=host).component_param(\n",
    "    loss=t.nn.CrossEntropyLoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for server\n",
    "nn_component.get_party_instance(role='arbiter', party_id=guest).component_param(    \n",
    "    trainer=trainer_param\n",
    ")\n",
    "\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()\n",
    "\n",
    "pipeline.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this script to submit the model, but submitting the model will take a long time to train and generate a long log, so we won't do it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
